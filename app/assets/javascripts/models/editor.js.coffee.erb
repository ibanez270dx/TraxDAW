####################################################
# TRAX.Editor
#   CoffeeScript "class" defining the Editor object.
#
####################################################

# Not Recording :: Microphone -> VolumeNode -> AnalyserNode -> ScriptProcessor -> Destination
#
# Recording :: Microphone -> VolumeNode -> AnalyserNode -> ScriptProcessor -> Recorder -> Destination

@TRAX ||= {}
@TRAX.Editor = class Editor

  track: undefined

  constructor: (editor, options) ->
    @editor = editor
    @liveWaveform = @editor.find('#live-waveform')

    @bufferSize = 2048
    @sampleRate = 44100
    @bufferSource = undefined
    @stream = undefined
    @currentTrackId = 0

    @notesInQueue = []
    @nextNoteTime = 0.0
    @current16thNote = 0
    @last16thNoteDrawn = -1
    @scheduleAheadTime = 0.1
    @lookahead = 25.0
    @timerID = 0
    @tempo = 60
    @noteResolution = 0   # 0 == 16th, 1 == 8th, 2 == quarter note
    @noteLength = 0.05

    [@micRunning, @isRecording, @isPlaying] = [false, false, false]
    [@width, @height, @cursorPosition] = [0, 0, 0]
    @chromaOrange = new chroma.scale(["#00ff00", "#e0e000", "#ff7f00", "#b60101"])
    
    @controls =
      power: @editor.find('nav li#power')
      microphone: @editor.find('nav li#microphone')
      metronome: @editor.find('nav li#metronome')
      record: @editor.find('nav li#record')
      playback: @editor.find('nav li#playback')

    # Setup Web Audio API
    @context = new webkitAudioContext()
    @_setupNodes()
    @_setupAudioHandler()

    # Create Workspace
    @workspace = new TRAX.Workspace(@context, @editor.find('#workspace'))

    # Create a track to start
    @workspace.createTrack()

    # Start Time Syncronization
    window.requestAnimFrame(@_drawMetronome)

    # Initialize Editor
    $('nav li.disabled input').prop('disabled', true)
    @_setupClickHandlers()

  ############################################################################################################
  # Web Audio API   
  ############################################################################################################

  _setupNodes: =>
    # Script Processer Node
    # This interface is an AudioNode which can generate, process, or analyse audio directly using JavaScript.
    # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#ScriptProcessorNode
    @scriptProcessor = @context.createScriptProcessor(2048, 2, 2)
    @scriptProcessor.connect(@context.destination)

    # Analyser Node
    # This interface represents a node which is able to provide real-time frequency and time-domain 
    # analysis information. The audio stream will be passed un-processed from input to output.
    # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#AnalyserNode-section
    @analyser = @context.createAnalyser()
    @analyser.smoothingTimeConstant = 0
    @analyser.fftSize = 512
    @analyser.connect(@scriptProcessor)

    # Gain Node
    # This interface is an AudioNode with a single input and single output. It multiplies the input audio 
    # signal by the (possibly time-varying) gain attribute, copying the result to the output. By default, 
    # it will take the input and pass it through to the output unchanged, which represents a constant gain 
    # change of 1.
    # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#GainNode
    @gain = @context.createGain()
    @gain.connect(@analyser)

  ############################################################################################################
  # Audio Processing   
  ############################################################################################################

  # AudioProcessingEvent Interface
  # This is an Event object which is dispatched to ScriptProcessorNode nodes. The event handler processes 
  # audio from the input (if any) by accessing the audio data from the inputBuffer attribute. The audio data 
  # which is the result of the processing (or the synthesized data if there are no inputs) is then placed into
  # the outputBuffer.
  # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#AudioProcessingEvent-section
  _setupAudioHandler: =>
    @scriptProcessor.onaudioprocess = (event) =>
      array = new Uint8Array(@analyser.frequencyBinCount)
      @analyser.getByteTimeDomainData(array)

      if @micRunning
        @_drawLiveWaveform(array) 

      if @isRecording
        # We clone the samples and set Track channels
        leftChannel = event.inputBuffer.getChannelData(0)
        rightChannel = event.inputBuffer.getChannelData(1)
        @workspace.currentTrack.leftChannel.push(new Float32Array(leftChannel))
        @workspace.currentTrack.rightChannel.push(new Float32Array(rightChannel))
        
        # Set Buffer Length and Track duration
        @workspace.currentTrack.length += event.inputBuffer.length
        @workspace.currentTrack.duration += event.inputBuffer.duration

        # Draw Canvas
        @workspace.currentTrack.draw(array)

      if @isPlaying
        console.log "it's playing!"

  window.requestAnimFrame = (->
    window.requestAnimationFrame or window.webkitRequestAnimationFrame or window.mozRequestAnimationFrame or window.oRequestAnimationFrame or window.msRequestAnimationFrame or (callback) ->
      window.setTimeout callback, 1000 / 60
  )()

  _drawMetronome: =>
    console.log "draw Metronome"
    currentNote = @last16thNoteDrawn
    currentTime = @context.currentTime
    while @notesInQueue.length and @notesInQueue[0].time < currentTime
      currentNote = @notesInQueue[0].note
      @notesInQueue.splice 0, 1 # remove note from queue
      # console.log 
    # We only need to draw if the note has moved.
    unless @last16thNoteDrawn is currentNote
      console.log "hit"
      # @controls['metronome'].find('.icon-metronome').addClass('active').delay(500).queue ->
        # $(this).removeClass('active').dequeue()
    #   x = Math.floor(canvas.width / 18)
    #   canvasContext.clearRect 0, 0, canvas.width, canvas.height
    #   i = 0

    #   while i < 16
    #     canvasContext.fillStyle = (if (currentNote is i) then ((if (currentNote % 4 is 0) then "red" else "blue")) else "black")
    #     canvasContext.fillRect x * (i + 1), x, x / 2, x / 2
    #     i++
    #   last16thNoteDrawn = currentNote
    
    # set up to draw again
    window.requestAnimFrame(@_drawMetronome)

  _scheduler: =>  
    # while there are notes that will need to play before the next interval, 
    # schedule them and advance the pointer.
    while @nextNoteTime < @context.currentTime + @scheduleAheadTime
      @_scheduleNote(@current16thNote, @nextNoteTime)
      @_nextNote()
    @timerID = window.setTimeout(@_scheduler, @lookahead)

  _scheduleNote: (beatNumber, time) =>
    # push the note on the queue, even if we're not playing.
    @notesInQueue.push
      note: beatNumber
      time: time

    return  if (@noteResolution is 1) and (beatNumber % 2) # we're not playing non-8th 16th notes
    return  if (@noteResolution is 2) and (beatNumber % 4) # we're not playing non-quarter 8th notes
    
    # create an oscillator
    ctx = @context.createOscillator()
    ctx.connect(@context.destination)
    if beatNumber % 16 is 0  # beat 0 == low pitch
      ctx.frequency.value = 220.0
    else if beatNumber % 4   # quarter notes = medium pitch
      ctx.frequency.value = 440.0
    else                     # other 16th notes = high pitch
      ctx.frequency.value = 880.0
    ctx.start(time)
    ctx.stop(time + @noteLength)
    
  _nextNote: =>
    # Advance current note and time by a 16th note...
    secondsPerBeat = 60.0 / @tempo   # Notice this picks up the CURRENT
    # tempo value to calculate beat length.
    @nextNoteTime += 0.25 * secondsPerBeat # Add beat length to last beat time
    @current16thNote++ # Advance the beat number, wrap to zero
    @current16thNote = 0  if @current16thNote is 16

  ############################################################################################################
  # HTML5 Canvases   
  ############################################################################################################

  _drawLiveWaveform: (array) =>
    ctx = @liveWaveform[0].getContext("2d")
    ctx.clearRect(0, 0, 100, 35)

    for i in [0..array.length]
      value = array[i]
      ctx.fillStyle = @chromaOrange(value / 255).hex()
      ctx.fillRect((i * 100) / 255, (value / 2) - 45, 1, 1)

  ############################################################################################################
  # Editor Methods   
  ############################################################################################################

  enableMicrophone: =>
    @controls['microphone'].removeClass('disabled')
    @controls['microphone'].find('input').prop('disabled', false)

  disableMicrophone: =>
    @controls['microphone'].addClass('disabled')
    @controls['microphone'].find('input').prop('disabled', true)

  startMicrophone: =>
    @micSource = @context.createMediaStreamSource(@stream)
    @micSource.connect(@gain)
    @micRunning = true
    @enableRecord()

  stopMicrophone: =>
    @micRunning = false
    @micSource.disconnect(@volume)    
    @liveWaveform.get()[0].getContext("2d").clearRect(0, 0, 100, 35)
    @disableRecord()

  enableMetronome: =>
    @controls['metronome'].removeClass('disabled')
    @controls['metronome'].find('input').prop('disabled', false)

  disableMetronome: =>
    @controls['metronome'].addClass('disabled')
    @controls['metronome'].find('input').prop('disabled', true)

  startMetronome: =>
    console.log "start metronome"
    @current16thNote = 0
    @nextNoteTime = @context.currentTime;
    @_scheduler()

  stopMetronome: =>
    console.log "stop metronome"
    window.clearTimeout(@timerID)

  enableRecord: =>
    @controls['record'].removeClass('disabled')
    @controls['record'].find('input').prop('disabled', false)

  disableRecord: =>
    @controls['record'].addClass('disabled')
    @controls['record'].find('input').prop('disabled', true)

  startRecord: =>
    if @micRunning
      @controls['record'].find('i.circle-on').addClass("active")
      @workspace.currentTrack.reset()
      @isRecording = true 

  stopRecord: =>
    @isRecording = false
    $("div.record i").removeClass("active")
    $('div.playback').removeClass('disabled')
    @workspace.currentTrack.saveBuffers()
    console.log "duration: ", @workspace.currentTrack.duration

  enablePlayback: =>
    @controls['playback'].removeClass('disabled')
    @controls['playback'].find('input').prop('disabled', false)

  disablePlayback: =>
    @controls['playback'].addClass('disabled')
    @controls['playback'].find('input').prop('disabled', true)

  startPlayback: =>
    $('div.playback i').addClass('active')
    @workspace.startPlayback()
    @isPlaying = true

  stopPlayback: =>
    $('div.playback i').removeClass('active')
    @workspace.stopPlayback()
    @isPlaying = false

  ############################################################################################################
  # Event Handling
  ############################################################################################################

  _setupClickHandlers: =>
    # Start Audio API
    @editor.find('input#power-on').on 'click', (event) =>
      target = $(event.currentTarget)
      if target.is(':checked')
        navigator.webkitGetUserMedia { audio: true }, (stream) =>
          @stream = stream
          @enableMicrophone()
          @enableMetronome()
        , (event) =>
          target.prop('checked', false)

    # Microphone Toggle
    @editor.find("input#microphone-on").on 'click', (event) =>
      if $(event.currentTarget).is(':checked')
        @startMicrophone()
      else
        @stopMicrophone()

    # Metronome Toggle
    @editor.find("input#metronome-on").on 'click', (event) =>
      if $(event.currentTarget).is(':checked')
        @startMetronome()
      else
        @stopMetronome()

    # Recording Toggle
    @editor.find("input#record-on").on 'click', (event) =>
      if $(event.currentTarget).is(':checked')
        @startRecord()
      else
        @stopRecord()

    # # Stop Recording
    # @editor.find("#rec-stop").on 'click', (event) =>
    #   @stopRecording()

    # # Start Playback
    # @editor.find("#play-start").on 'click', (event) =>
    #   @startPlayback()

    # # Stop Playback
    # @editor.find('#play-stop').on 'click', (event) =>
    #   @stopPlayback()

    # Add Track
    @editor.find('#add-track').on 'click', (event) =>
      event.preventDefault()
      @workspace.createTrack()

    # Change Track Name
    $(document).on 'click', '.track .title', (event) =>
      event.preventDefault()

