####################################################
# TRAX.Editor
#   CoffeeScript "class" defining the Editor object.
#
####################################################

# Not Recording :: Microphone -> VolumeNode -> AnalyserNode -> ScriptProcessor -> Destination
#
# Recording :: Microphone -> VolumeNode -> AnalyserNode -> ScriptProcessor -> Recorder -> Destination

@TRAX ||= {}
@TRAX.Editor = class Editor

  track: undefined

  constructor: (editor, options) ->
    @editor = editor
    @liveWaveform = @editor.find('#live-waveform')

    @bufferSize = 2048
    @sampleRate = 44100
    @bufferSource = undefined
    @currentTrackId = 0

    [@micRunning, @isRecording, @isPlaying] = [false, false, false]
    [@width, @height, @cursorPosition] = [0, 0, 0]
    @chromaOrange = new chroma.scale(["#00ff00", "#e0e000", "#ff7f00", "#b60101"])
    
    # Setup Web Audio API
    @context = new webkitAudioContext()
    @_setupNodes()
    @_setupAudioHandler()

    # Initialize Editor
    @buffer = new TRAX.Buffer(@context)
    @_setupClickHandlers()

    # Create Workspace
    @workspace = new TRAX.Workspace(@context, @editor.find('#workspace'))

    # Create a track to start
    @workspace.createTrack()


  ############################################################################################################
  # Web Audio API   
  ############################################################################################################

  _setupNodes: =>
    # Script Processer Node
    # This interface is an AudioNode which can generate, process, or analyse audio directly using JavaScript.
    # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#ScriptProcessorNode
    @scriptProcessor = @context.createScriptProcessor(2048, 2, 2)
    @scriptProcessor.connect(@context.destination)

    # Analyser Node
    # This interface represents a node which is able to provide real-time frequency and time-domain 
    # analysis information. The audio stream will be passed un-processed from input to output.
    # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#AnalyserNode-section
    @analyser = @context.createAnalyser()
    @analyser.smoothingTimeConstant = 0
    @analyser.fftSize = 512
    @analyser.connect(@scriptProcessor)

    # Gain Node
    # This interface is an AudioNode with a single input and single output. It multiplies the input audio 
    # signal by the (possibly time-varying) gain attribute, copying the result to the output. By default, 
    # it will take the input and pass it through to the output unchanged, which represents a constant gain 
    # change of 1.
    # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#GainNode
    @gain = @context.createGain()
    @gain.connect(@analyser)

  ############################################################################################################
  # Audio Processing   
  ############################################################################################################

  # AudioProcessingEvent Interface
  # This is an Event object which is dispatched to ScriptProcessorNode nodes. The event handler processes 
  # audio from the input (if any) by accessing the audio data from the inputBuffer attribute. The audio data 
  # which is the result of the processing (or the synthesized data if there are no inputs) is then placed into
  # the outputBuffer.
  # https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html#AudioProcessingEvent-section
  _setupAudioHandler: =>
    @scriptProcessor.onaudioprocess = (event) =>
      array = new Uint8Array(@analyser.frequencyBinCount)
      @analyser.getByteTimeDomainData(array)

      if @micRunning
        @_drawLiveWaveform(array) 

      if @isRecording
        # We clone the samples and set Track channels
        leftChannel = event.inputBuffer.getChannelData(0)
        rightChannel = event.inputBuffer.getChannelData(1)
        @track.leftChannel.push(new Float32Array(leftChannel))
        @track.rightChannel.push(new Float32Array(rightChannel))
        
        # Set Buffer Length and Track duration
        @track.length += event.inputBuffer.length
        @track.duration += event.inputBuffer.duration

        # Draw Canvas
        @_drawRecording(array)

      if @isPlaying
        console.log "it's playing!"

  ############################################################################################################
  # HTML5 Canvases   
  ############################################################################################################

  _drawLiveWaveform: (array) =>
    ctx = @liveWaveform[0].getContext("2d")
    ctx.clearRect(0, 0, 100, 35)

    for i in [0..array.length]
      value = array[i]
      ctx.fillStyle = @chromaOrange(value / 255).hex()
      ctx.fillRect((i * 100) / 255, (value / 2) - 45, 1, 1)

  _drawWaveform: (array) =>
    ctx = @track['canvases']['wave'][0].getContext("2d")
    waveValues = []
    min = Math.min.apply(Math, array) * 100 / 255
    max = Math.max.apply(Math, array) * 100 / 255
    while min < max
      waveValues.push(min)
      min++

    for i in [0..waveValues.length]
      value = waveValues[i]
      ctx.fillStyle = "#{@track.getHexColor()}"
      ctx.fillRect(0, value, 1, 1)
    ctx.translate(1, 0)

  _drawBackground: =>
    ctx = @track['canvases']['background'][0].getContext("2d")
    ctx.fillStyle = "rgba(255,255,255,0.1)"
    ctx.fillRect(0, 0, 1, @track['height'])
    ctx.translate(1, 0)
    @cursorPosition++

  _drawRecording: (array) =>
    @_drawWaveform(array)
    @_drawBackground()

  ############################################################################################################
  # Editor Methods   
  ############################################################################################################

  startMicrophone: =>
    navigator.webkitGetUserMedia { audio: true }, (stream) =>
      @micSource = @context.createMediaStreamSource(stream)
      @micSource.connect(@gain)
      @micRunning = true
      $("nav .record").removeClass("disabled")

  stopMicrophone: =>
    @micSource.disconnect(@volume)
    @micRunning = false
    @liveWaveform.get()[0].getContext("2d").clearRect(0, 0, 100, 35)
    $("nav .record").addClass("disabled")

  startRecording: =>
    if @micRunning
      $("div.record i").addClass("active")
      @track.reset()
      @isRecording = true 

  stopRecording: =>
    @isRecording = false
    $("div.record i").removeClass("active")
    $('div.playback').removeClass('disabled')
    @track.saveBuffers()
    console.log "duration: ", @track.duration

  startPlayback: =>
    $('div.playback i').addClass('active')
    @buffer.play(@tracks)
    @isPlaying = true

  stopPlayback: =>
    $('div.playback i').removeClass('active')
    @buffer.stop()
    @isPlaying = false

  ############################################################################################################
  # Event Handling
  ############################################################################################################

  _setupClickHandlers: =>
    # Start Microphone
    @editor.find("#mic-start").on 'click', (event) =>
      @startMicrophone()

    # Stop Microphone
    @editor.find("#mic-stop").on 'click', (event) =>
      @stopMicrophone()

    # Start Recording
    @editor.find("#rec-start").on 'click', (event) =>
      @startRecording()

    # Stop Recording
    @editor.find("#rec-stop").on 'click', (event) =>
      @stopRecording()

    # Start Playback
    @editor.find("#play-start").on 'click', (event) =>
      @startPlayback()

    # Stop Playback
    @editor.find('#play-stop').on 'click', (event) =>
      @stopPlayback()

    # Add Track
    @editor.find('#add-track').on 'click', (event) =>
      event.preventDefault()
      @workspace.createTrack()

    # Change Track Name
    $(document).on 'click', '.track .title', (event) =>
      event.preventDefault()

    # Select Track
    $(document).on 'click', '#workspace .controls', (event) =>
      event.preventDefault()
      track_id = $(event.currentTarget).data('track-id')
      @workspace.selectTrackById(track_id)

